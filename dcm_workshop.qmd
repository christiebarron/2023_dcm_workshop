---
title: "2023-04-18 DCM Workshop"
format: html
editor: visual
bibliography: references.bib
eval: true
toc: true
toc-depth: 3
toc-expand: 3
number-sections: true
number-depth: 3
embed-resources: true
---

![](fig/Slide1.png)

This workshop is a synthesis of the phenomenal work by academics in the field of DCM. The NCME ITEMS modules by [@ma2019] and [@carragher2019] and NCME DCM workshops [@madison2023] inspired a lot of this content. As did a variety of other sources [@george2015; @shi2021; @rupp2017; @handbook2019; @ma2020a; @delatorre2019a]

## Preliminaries

### Outline

-   Conceptual foundations of DCM
    -   Conceptual ideas of DCM models
-   Running DCM in R
    -   Reading in data and q-matrix
    -   Empirical q-matrix validation
    -   Measurement model calibration
    -   Model fit evaluation
    -   Item diagnosticity
    -   Classification Reliability
    -   Parameter Estimate Interpretation
    -   Saving and Reporting DCM
-   Wrap-up and Next Steps
    -   Advanced Applications of DCM
    -   Summary
    -   Resources

### Learning Goals

Upon leaving this workshop, you will be able to:

-   Describe the basic principles of DCM, and circumstances where it may be beneficial to use

-   Run and interpret a DCM using the GDINA R package

-   Specify various item-specific DCM models as special cases of a general DCM model

-   Investigate model diagnostics using several statistical procedures

-   Save DCM analysis results

## Conceptual Foundations of DCM

Characteristics of DCM include:

-   Criterion referenced

-   Categorical Latent Variable predicting/causing performance on indicators

### Continuous Latent Variable

-   Imagine a coach wants to assess tennis ability. Some approaches available include:

    -   obtain a test score (# of items correct) on a test (Classical Test Theory)

    -   obtain a latent ability estimate using test scores (IRT or categorical CFA)

-   Both of these approaches are *normative*, providing information about ability location along a continuum:

![](ppt/Slide2.png)

-   These approaches allow rank ordering of Christie \< Liam \< Eunice in tennis ability.

    -   They also allow for statements such as: Christie is at the 2.5th percentile, Liam got an average score, Eunice has a high level of performance, etc.

-   But why are Christie and Liam low on tennis ability? What skills do they need to work on? How can we help them get better?

### Categorical Latent Variable

-   Instead of focusing on overall tennis ability, we could instead focus on the skills (i.e., attributes) required for playing tennis: technique, endurance, agility, and strategy.

![](fig/Slide3.png)

-   These skills are categorical rather than continuous, differentiating between whether players are proficient (i.e., have mastered a skill) or non-proficient (i.e., still developing a skill)

### Proficiency/Mastery Profiles

-   Item responses can then be used to place players into groups based on their status as proficient or non-proficient on the multiple attributes:

+-------------+----------------+----------------+----------------+----------------+
| Player      | Technique      | Endurance      | Agility        | Strategy       |
+=============+:==============:+:==============:+:==============:+:==============:+
| Christie    | Non-proficient | Non-proficient | Non-proficient | Non-proficient |
+-------------+----------------+----------------+----------------+----------------+
| Liam        | Non-proficient | Proficient     | Non-proficient | Non-proficient |
+-------------+----------------+----------------+----------------+----------------+
| Eunice      | Proficient     | Proficient     | Proficient     | Proficient     |
+-------------+----------------+----------------+----------------+----------------+
| Serena      | Proficient     | Proficient     | Proficient     | Proficient     |
+-------------+----------------+----------------+----------------+----------------+

-   We now have a better sense of the skills different players have mastered. We see Liam is lower than Eunice because he isn't proficient in Technique or Strategy yet, and Christie hasn't become proficient in anything yet.

    -   We also see that Eunice and Serena, despite being a standard deviation apart, have the same proficiency profile. This is because DCMs do not differentiate within-class.

-   This *diagnostic* description of players' strengths and areas for improvement is a cornerstone of DCM.

-   DCM can be applied to Education (proficiency in cognitive skills), clinical psychology (exhibition of psychopathology), organizational psychology (proficiency in occupational skills), and personality psychology (e.g., exhibition of mastery goal orientation), among others.

## Statistical Characteristics of DCM Models

-   Like latent class analysis and latent profile analysis, DCMs are a latent-class based set of psychometric models [@rupp2017].

-   Perforamnce is based on multiple discrete latent variables (attributes) -- similar to Multidimeinsiaonal IRT.

-   Rather than theta estimates or factor scores, the major output in a DCM is a multi-faceted classification of learners into latent classes.

-   DCM items are designed to measure 1+ latent attributes. A Q-matrix is used to specify which attributes are required for a given item.

-   Like IRT and CFA, the observed response to an item is expected to depend on the latent construct. Separate response probabilities are expected for every latent class.

-   Reliability estimates for discrete score variabales that yield the classifications can still be computed. They are DCM-specific formulas though

### DCM inputs and Outputs

[@carragher2019] conceptualize DCM as having certain inputs and outputs:

Inputs:

1.  Performance Data

2.  Q-matrix

Outputs:

3.  Item statistics

4.  Person statistics: individual level

5.  Person statistics: group level

![reprinted](fig/output-indiv-person.png){alt="reprinted"} (reprinted from [@carragher2019])

### Continuous versus Categorical models

-   Ma and de la Torre [@ma2019] provide a helpful distinction between multidimensional IRT and DCM.
-   Within the multidimensional IRT framework with two variables, the 3-dimensional modeled surface would look like: ![](fig/mirt.png)

While a diagnostic classification model with 2 attributes would look like: ![](fig/2feature_dcm.png)

### DCM models

-   Over the years, a plethora of DCM models have been developed, each with different assumptions and capabilities:

    ![](fig/acronym-meme.jpeg)

Several resources have provided thorough descriptions of DCM models [@carragher2019; @ma2019; @ravand2020]. They describe each model as having certain conceptual properties:

| Model     | Attribute Assumption                         | Specific vs General |
|-----------|----------------------------------------------|---------------------|
| DINA      | Non-Compensatory                             | Specific            |
| DINO      | Compensatory                                 | Specific            |
| A-CDM     | Additive                                     | Specific            |
| LLM/C-RUM | Additive                                     | Specific            |
| R-RUM     | Additive                                     | Specific            |
| G-DINA    | Compensatory, Non-Compensatory, and Additive | General             |

Each of these models also has statistical properties:

| Model     | Link Function |             Outcome              | Parameters Estimated                                       |
|-----------|:-------------:|:--------------------------------:|------------------------------------------------------------|
| DINA      | Identity Link |       Response Probability       | intercept (guessing), highest-order interaction (slipping) |
| DINO      | Identity Link |       Response Probability       | intercept (guessing), single main effect (slipping)        |
| A-CDM     | Identity Link |       Response Probability       | intercept, main effects                                    |
| LLM/C-RUM |  Logit Link   | Log-odds of Response Probability | intercept, main effects                                    |
| R-RUM     |   Log Link    |   Log of Response Probability    | intercept, main effects                                    |
| G-DINA    | All as needed |              Varies              | intercept, main effects, all interactions                  |

#### Plots of Restricted DCM models

Correspondingly, each model has a prototypical item response function. DINA and DINO models look like the following:

![](fig/dina_dino.png)

A-CDM, LLM, C-RUM, and R-RUM are characterized by mastery of attributes combining in an additive manner as shown below. They differ in the link function. A-CDM is shown, reprinted from [@ma2019]:

![](fig/acdm.png)

### General DCM Models

-   Conceptually similar to how the t-test, ANOVA, ANCOVA, and linear regression can be subsumed under the general linear model, more general DCM models that subsume the others have been developed.

-   Unified modeling frameworks include the G-DINA, log-linear cognitive diagnosis model (LCDM), and general diagnostic model (GDM).

    -   They are complicated models with lots of parameters that subsume other models.

    -   For instance, the G-DINA model subsumes several other models:

![](fig/dcm-models.png)

### General DCM Model: Conceptual Example

Imagine we want to run an ANOVA with three dichotomous indicator variables and a continuous outcome.

-   Main effects model would have an intercept, main effect 1, main effect 2, main effect 3.

    -   Two-way interaction model would have intercept, main effect 1, main effect 2, main effect 3, 1\*2, 2\*3, 1\*3

    -   Three-way interaction model would have intercept, main effect 1, main effect 2, main effect 3, 1\*2, 2\*3, 1\*3, 1\*2\*3.

-   Conceptually, General DCMs are the highest-order interaction model. If the interaction is 0, it simplifies down to simpler model.

    -   We value parsimony, and want to avoid overfitting. So if the interaction is 0, we want to use a simpler model. We'll describe further in `Step 2: Model Calibration` of this workshop.

    -   For an accessible description of the statistics behind DCM models, see [@ma2019] and [@carragher2019].

-   When models are nested, straightforward model comparison is possible using likelihood ratio tests using the R `anova()` function.

## DCM Capabilities in R

### DCM R packages

There are several fantastic R packages for DCM. Examples include:

+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+
| R Package                                                                | Description                                                                                                 | Citation                                                            |
+==========================================================================+:===========================================================================================================:+:===================================================================:+
| [GDINA](https://wenchao-ma.github.io/GDINA/index.html)                   | Model estimation & fit, q-matrix estimation & validation, and more                                          | [@ma2020 @delatorre2019]                                            |
+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+
| [CDM](https://cran.r-project.org/web/packages/CDM/CDM.pdf)               | Estimation, q-matrix validation, fit, and more                                                              | [@george2016]                                                       |
+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+
| [cdmTools](https://cran.r-project.org/web//packages/cdmTools/index.html) | Estimate small-sample DCM models, empirical q-matrix estimation, empirical number of attributes calculation | [link](https://cran.r-project.org/web/packages/cdmTools/index.html) |
+--------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+

Many other software options are available. Mplus can handle DCM with a lot of tedious manual coding, as can SAS. The [tdcm r package]() is great with longitudinal DCM. Many others I haven't mentioned are also useful.

### Dataset used in this workshop

-   This workshop follows in using an open-source dataset widely available in many DCM R packages. The dataset is the grammar session of the Examination for the Certificate of Proficiency in English (ECPE).

George and Robitzsch (@george2015) provide the following example item from the ECPE:

> Mary had to lean \_\_ the counter to open the window.
>
> a)  above
> b)  over
> c)  after
> d)  around

The following table---reprinted from (@george2015 p. 191)\--provides a description of the three attributes in the current dataset:

+---------------------+-----------------------+----------------------------------------------------------------------------------------------+
| Attribute Parameter | Attribute Name        | Attribute Description                                                                        |
+=====================+:=====================:+:============================================================================================:+
| α1                  | morphosyntactic rules | word formation; combination of words into larger units such as phrases and sentences         |
+---------------------+-----------------------+----------------------------------------------------------------------------------------------+
| α2                  | cohesive rules        | grammatical and lexical linking within a text or sentence                                    |
+---------------------+-----------------------+----------------------------------------------------------------------------------------------+
| α3                  | lexical rules         | modification of argument structures of lexical text elements (i. e., verbs and declensions)  |
+---------------------+-----------------------+----------------------------------------------------------------------------------------------+

### Install Dependencies

```{r data}
#a shortcut for loading and/or installing all required packages
if (!require(pacman)) install.packages("pacman")
pacman::p_load(janitor, tidyverse, GDINA, psych, psychometric)
```

## Running DCM in R with ECPE Data

This workshop will go through using R syntax. You can instead use the GDINA graphical user interface by opening base R, loading GDINA `library(GDINA)`, loading shinydashboard `library(shinydashboard)`, and running `GDINA::startGDINA()`. See [@ma2019] on how to use the GINDA graphical user interface.

### Step Disclaimers

Note: these steps assume a large degree of qualitative and theoretical work has already gone into the measure development. For example, construct implicit theory, consult experts, construct q-matrix, construct test, expert judges & think-aloud to pilot test, etc [@ravand2020].

These 'steps' are also less linear practice; revisions are often required. There may be other, equally acceptable model building practices too.

### Step 0: Preliminaries

#### Reading in data and q-matrix

Start by reading in the data, reading in the q-matrix, and cleaning column names:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#read in the performance data and clean the column names for easier use with three steps
# 1) read in data with readr's read_csv function
# 2) use janitor's clean_names() function to make column names easier to work
# 3) convert from tidyverse's default tibble to R's basic data.frame object for GDINA

ecpe_df <- readr::read_csv(file = "raw_data/ecpe_grammar.csv") %>%  
  janitor::clean_names() %>%   
  as.data.frame() 

#save data with id column
ecpe_id <- ecpe_df

#remove id from data frame for analyses using dplyr::select() function
ecpe_df <- dplyr::select(ecpe_df, -id)


#read in the q-matrix and clean the column names
ecpe_q <- readr::read_csv(file = "raw_data/ecpe_qmatrix.csv") %>% 
  janitor::clean_names() 

# create a vector of q-matrix names (see Landshaw, 2017 within Rupp and Leighton 2017 or george et al., 2015)
attributes <- c("morphosyntactic", "cohesive", "lexical") 

#save the attributes as column names (overwriting the default "a1") using the vector of q-matrix names
names(ecpe_q) <- attributes

#create vector of item names for future use
item_vec <- colnames(ecpe_df)
```

```{r}
#print first 5 rows to show that it worked
head(ecpe_q)


```

Note that for any of these functions, can use `help()` or `?` before a function get get information about it.

```{r}
#getting more information about the dplyr::select() function as an example
help(select, package = "dplyr")
?dplyr::select()
```

#### Data Descriptives

Get basic descriptives about the data:

```{r}
#get the structure of the data with str()
str(ecpe_df)

#get summary statistics of the data with summary(). assumes continuous variables
summary(ecpe_df)

#view the first 5 rows of the data with head().
head(ecpe_df)

#get descriptive statistics (that assume continuous variables) using psych package's describe() function. For brevity, only keep the last 5 rows using tail()
tail(psych::describe(ecpe_df))

```

It is probably a good idea to also run basic CTT to get a sense of how the items are performing. the `psychometric` package has a handy `item.exam()` function. These analyses can't handle an ID variable, so it needs to be removed prior to analyzing.

```{r}
#use the psychometric package to run CTT as prelim exploration of the data. 
psychometric::item.exam(ecpe_df, discrim=TRUE)[,c(1, 3, 4, 5, 8)] #item-total, item discrim, item difficulty
```

### Estimate a DCM in R with GDINA

We can run a DCM model using the GDINA package's `GDINA()` function. The general notation is `GDINA::GDINA(dat = your_dataframe, Q = your_qmatrix, model = models_to_use)`. See [Wenchao Ma's vignettes](https://cran.r-project.org/web/packages/GDINA/vignettes/GDINA.html) and [@shi2021] for additional information on using the GDINA R package.

Its good practice to start with a fully saturated DCM model for empirical q-matrix validation[^1]. So we'll specify "GDINA" as the item-specific DCM model every item should use:

```{r echo=TRUE, message = FALSE, warning = FALSE, error = FALSE}

#run the model by giving GDINA() the dataframe, q-matrix, and item-level model to use
ecpe_est <- GDINA::GDINA(dat = ecpe_df, Q = ecpe_q, model = "GDINA") #, mono.constraint = TRUE) 

```

```{r}
#print summary information about the model
print(ecpe_est)

#get summary information about the model
summary(ecpe_est)
```

One design decision is whether to add a monotonicity constraint. Monotonicity constraint means that the model forces the probability of getting an item correct to not decrease as more skills are mastered. This is often theoretically justifiable, and can help improve model estimation, especially in small samples [@shi2021]. But, keep an eye on boundary estimates; these may indicate poor estimation. We can add monotonicity constraints with an additional `mono.constraint = TRUE` argument:

```{r echo = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#adding a monotonicity constraint
ecpe_est <- GDINA::GDINA(dat = ecpe_df, Q = ecpe_q, model = "GDINA", mono.constraint = TRUE) 

```

The output of the `GDINA` function that we saved as `gdina_est` is a list with lots of information about the model. We can see the elements of the list with a few functions:

```{r}
#get high-level info about gdina list object
summary(ecpe_est)

#get more fine-grained information about the list. change max.level argument for even more information.
str(ecpe_est, max.level = 1)
```

That's a lot! Fortunately, `GDINA` provides several methods for extracting information we need from this list. Use `?GDINA::extract()` for a comprehensive list of options. `stats::coef()` can be used to extract item information, and `GDINA::personparm()` can be used to extract person information.

[^1]: One exception is when there are simply too many interaction effects for adequate estimation or sample size For instance, an item requiring four or five latent attributes would have a large number of parameters. Ignoring the highest-order factor in these cases may be justifiable [@madison2023].

### Step 1: Empirical Q-matrix Validation

When there is uncertainty about the accuracy of the q-matrix, empirical Q-matrix validation is a useful technique [@shi2021]. It involves running statistical procedures to determine what attributes an item is capturing. A note of caution: these should be used to inform theoretically justified q-matrix revision, not to replace thoughtful q-matrix development by context experts.

For additional information about pragmatrics of R for q-matrix validation, see [Wenchao Ma's GDINA vignette](https://wenchao-ma.github.io/GDINA/articles/OnlineExercises/CDMAnalysis_example.html). There are also several articles on the topic.

There are multiple empirical q-matrix options for empirical q-matrix validation. R packages GDINA, CDM, NPCD, and cdmTools all have functions for it. We will focus on the GDINA R package:

```{r}

#default is de la Torre and Chiu’s (2016) algorithm for q-matrix validation.
ecpe_qval <- Qval(ecpe_est, method = "PVAF")
ecpe_qval
#PVAF stands for "the proportion of variance accounted for"

#use an iterative procedure (test-attribute level) described by Najera et al., (2019, 2020)
ecpe_qval <- Qval(ecpe_est, method = "PVAF", eps = -1,
              iter = "test.att", iter.args = list(verbose = 1))
ecpe_qval

#method = "Wald" uses a stepwise method (Ma & de la Torre, 2019).
ecpe_qval <- Qval(ecpe_est, method = "Wald")
ecpe_qval
```

To my knowledge, it is still unclear which option of the above are best. Perhaps more useful is visual inspection of a MESA plot. ![a literal mesa plot](fig/mesa.jpeg){alt="a literal mesa plot"}. In (statistical) mesa plots, the x-axis represents the q-vectors (which attributes are hypothesized to be required in answering a given item) and the y-axis corresponding to their PVAF values. Kind of like a scree plot, the q-vector on the edge of the "mesa" can be considered the correct q-vector for the item. In the GDINA package, the red dot in these show the q-vector that was originally hypothesized.

```{r}
#Create a MESA plot

#note: can replace c(1,9) with c(1:nrow(ecpe_q)) as a shorthand for all items
plot(ecpe_qval, item = c(1,9)) 

```

The suggested q-matrix based on the `Qval` function can be extracted.

Once thoroughly inspecting the empirical q-matrix information *and* considering their theoretical justification, a final q-matrix can be decided. Let's assume the suggested revisions make sense. We can then save the extracted q-matrix to run the DCM estimation again:

```{r}
#extract and save the suggested Q-matrix using the extract function
ecpe_q_empirical <- GDINA::extract(ecpe_qval, what = "sug.Q")

#alternatively, can extract and save the suggested Q-matrix from the list directly
ecpe_q_empirical <- ecpe_qval[["sug.Q"]]

#If there were some suggestions we want to keep, and some we want to discard, we can manually make that change instead:
#ecpe_q_empirical <- ecpe_q
#ecpe_q_empirical[3,] <- as.list(c(1, 0, 0)) #extract the 3rd row of q-matrix and replace it with our revised q.

#print the revised q
ecpe_q_empirical
```

Re-run the model with the suggested q-matrix and compare the model fit between the two q-matrices: original and revised

```{r eval = TRUE, message = FALSE, warning = FALSE, error = FALSE}
#run a model with the suggested q-matrix
ecpe_est2 <- GDINA::GDINA(ecpe_df, ecpe_q_empirical, mono.constraint = TRUE)
```

```{r}
#compare the fit of the model with original q-matrix (est) and revised q-matrix (est2)
anova(ecpe_est2,ecpe_est)
```

This is a very close decision. BIC and AIC differ, and the p-value is so close as to .05 to be unhelpful. Given the concordance between BIC and the p-value, let's keep the more parsimonious model.

### Step 2: Model Calibration

#### Measurement model calibration

With a "finalized" q-matrix, we can turn to ensuring a correctly specified DCM measurement model.

We will place constraints on the LCDM via *nested model forms*, then compare the constrained model to the saturated DCM. This can be planned a-priori, or can be empirical. Note that this process only applies to complex structure items; for simple structure items, all DCMs are equivalent.

GDINA package offers a function, `GDINA::modelcomp()`, which implements the Wald test and likelihood ratio test for assessing whether the G-DINA model can be reduced to five commonly used reduced models, namely, the DINA model, the DINO model, A-CDM, LLM, and R-RUM. Note that trivial discrepancies may be detected with large sample sizes, which is a limitation of this approach.

```{r}
#first, informally inspect whether any items may not require all interaction effects (based on patterns of probabilities) 
GDINA::extract(ecpe_est2, what = "catprob.parm")

#informally inspect plot
plot(ecpe_est2, withSE = TRUE)

```

Conduct a more formal model comparison:

```{r}
#conduct the model comparison. By default uses the wald test
ecpe_model_compare <- GDINA::modelcomp(ecpe_est2)
ecpe_model_compare
```

This suggests some items may not need the fully saturated GDINA model for adequate fit.

We can then save the recommended item-specific models to subsequently re-run the model with the suggested item-level DCMs

```{r}
#extract the recommended models from the model using the extract function
ecpe_model_items <- GDINA::extract(ecpe_model_compare, what = "selected.model")[["models"]]

#alternatively, manually extract the models from the list
ecpe_model_items <- ecpe_model_compare$selected.model$models
```

Estimate a third, more parsimonious DCM model and compare model fit between the two DCMs:

```{r}
#run the new DCM model with more parsimonious item-specific DCM measurement model
ecpe_est3 <- GDINA::GDINA(dat = ecpe_df, Q = ecpe_q_empirical, model = ecpe_model_items, mono.constraint = TRUE)

#compare the item-level model (est3) with fully saturated model (est2)
anova(ecpe_est3, ecpe_est2)

```

The more parsimonious DCM with item-specific DCM measurement models fits the data as well as the fully saturated DCM.

As a brief aside, I believe the Log-Linear CDM as a general model may make this process a little easier. If type I error rate is adequately controlled for, we can simply check the parameter estimates and associated p-values of the main effects and interaction effects. One can accomplish this with the CDM package, among others.

Why ensure model parsimony where possible? Sen and Cohen (2020) describe:

> "More complex DCMs require larger sample sizes to yield accurate estimates and more reduced DCMs can usually be estimated accurately with smaller sample sizes. Reduced models also typically provide for more straightforward interpretations and higher correct classification rates than more saturated models, particularly when the sample sizes are small." [@sen2021, p. 2]

### Step 3: Assess Model-Data Fit

One the empirical q-matrix and DCM measurement models have been selected, model fit is investigated. Like in SEM, there are various measures of model fit.

As described by [@shi2021], the `GDINA` R package has several options for investigating model fit:

+---------------+-----------------+------------------------------+---------------------------------------+
| Type of fit   | Level of fit    | Function                     | Statistics Provided                   |
+===============+:===============:+:============================:+:=====================================:+
| Absolute      | test level      | `modelfit()` and `itemfit()` | M2, RMSEA2, SRMSR, MaxAD.r, MaxAD.LOR |
+---------------+-----------------+------------------------------+---------------------------------------+
| Relative      | test level      | `modelfit()` and `anova()`   | AIC, BIC, CAIC, SABIC, LR test        |
+---------------+-----------------+------------------------------+---------------------------------------+
| Absolute      | item level      | `itemfit()`                  | MaxAD.r, MaxAD.LOR                    |
+---------------+-----------------+------------------------------+---------------------------------------+
| Absolute      | item-pair level | `itemfit()`                  | MaxAD.r, MaxAD.LOR                    |
+---------------+-----------------+------------------------------+---------------------------------------+
| Relative      | item level      | `modelcomp()`                | Wald test, LR test                    |
+---------------+-----------------+------------------------------+---------------------------------------+

#### Investigating test-level fit

There are a variety of statistics to assess test-level absolute fit. The table below provides a brief summary of interpretation.

+---------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Statistic     | Good Fit Cutpoint                                                                           | Description/Context                                                                                                                                                  |
+===============+=============================================================================================+======================================================================================================================================================================+
| M2            | `P > .05`: good fit.                                                                        | Compare bi/univariate distributions of observations to model predictions. Similar to chi-square in SEM. Like chi-square, often overpowered and thus not informative. |
|               |                                                                                             |                                                                                                                                                                      |
|               | A significant value indicates model predictions significantly depart from the observed data |                                                                                                                                                                      |
+---------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| RMSEA2        | `< .03`: excellent fit                                                                      | Limited-information RMSEA. Can be used as an effect size measure                                                                                                     |
|               |                                                                                             |                                                                                                                                                                      |
|               | `.03 - .045`: good fit                                                                      |                                                                                                                                                                      |
|               |                                                                                             |                                                                                                                                                                      |
|               | `> .045`: poor fit                                                                          |                                                                                                                                                                      |
+---------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| SRMSR         | `< .05`: good fit                                                                           | Standardized root mean square residual. Can be used as an effect size measure                                                                                        |
|               |                                                                                             |                                                                                                                                                                      |
|               | `> .05`: poor fit                                                                           |                                                                                                                                                                      |
+---------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| MaxAD.r       | `adj p > .05`: good fit                                                                     | The maximum absolute difference between observed and predicted Fisher transformed correlations (MaxAD.r). Aggregated item-level fit.                                 |
|               |                                                                                             |                                                                                                                                                                      |
|               | Estimates closer to 0: good fit                                                             | Investigates to what extent the model can explain the association between each pair of items.                                                                        |
+---------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| MaxAD.LOR     | `adj p > .05`: good fit                                                                     | The maximum absolute difference between observed and predicted log odds ratios (MaxAD.LOR). Aggregated item-level fit.                                               |
|               |                                                                                             |                                                                                                                                                                      |
|               | Estimates closer to 0: good fit                                                             | Investigates to what extent the model can explain the association between each pair of items.                                                                        |
+---------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+

== note to self: find a way to save and access the results better for this below==

```{r}
#extract the test-level model data fit using the modelfit function, then print results:
ecpe_tfit3 <- GDINA::modelfit(ecpe_est3)
ecpe_tfit3 

#extract aggregated item fit as a measure of test-level fit using itemfit() function
ecpe_ifit3 <- itemfit(ecpe_est3)
ecpe_ifit3
```

Test-level relative fit is available with the same object, and can be compared with other models as we have already done.

```{r}
#relative fit statistics are printed in the first part of this output
print(ecpe_tfit3)

#Compare all models thus far. Note the p-values compare all models to the original ecpe_est model (might not be helpful)
ecpe_tfit_relative <- stats::anova(ecpe_est, ecpe_est2, ecpe_est3)
```

#### Item-level and Item-pair Fit

Absolute measures of item-pair fit include the maximum absolute difference between observed and predicted 1) Fisher transformed correlation and 2) log odds ratios.

```{r}
#Extract the item pair fisherz and log odds
head(ecpe_ifit3$r)
head(ecpe_ifit3$logOR)

#note: used head() just to keep the output succinct.

```

Given the large amount of data in the above tests, we can instead interpret the item-pairs using a heatmap plot of the item fit object. In this plot, item-pairs with significant p values for the MaxAD.r and MaxAD.LOR are in red as they indicate significant misfit (i.e., discrepancies between observed and model-implied relationships between items)

```{r}
#note this heatmap plot is of the p-values for the MaxAD.r and MaxAD.LOR.
plot(ecpe_ifit3)
```

If the model fits the data, we can continue on to investigating item diagnosticity. If it does not fit the data, the measure, q-matrix, or measurement model needs to be refined.

#### Automating Steps 1-3

It is best to conduct steps 1-3 using theory and content experts' knowledge about the constructs. However, if we do do want to do steps 1-3 automatically, the `GDINA::autoGDINA()` function can be used.

### Step 4: Item Diagnosticity

The item characteristic bar chart (ICBC) can be used to informally determine how diagnostic an item is. There should be clear differences in item probability between students demonstrating proficiency and non-proficiency.

In the following charts, item 17 is an example of poor diagnosticity as all probabilities are quite close to each other. A student with non-proficiency has more than 75% chance of getting the item correct, and there is minimal difference in probability between non-proficient and proficient students. In comparison, items 12 and 20 are examples of good diagnosticity.

```{r}
plot(ecpe_est3, withSE = TRUE)
```

The `GINDA` package provides two formal options for determining how "diagnostic" an item is.

+-------------+---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Statistic   | Cutpoint for good diagnosticity       | Description                                                                                                                                                                           |
+=============+=======================================+=======================================================================================================================================================================================+
| *P(1)-P(0)* | Higher means item is more diagnostic. | Is the difference in success probabilities between those who are proficient in all required attributes *P(1)* and those who are proficient in none of the required attributes *P(0)*. |
|             |                                       |                                                                                                                                                                                       |
|             | Cutpoint undetermined.                |                                                                                                                                                                                       |
+-------------+---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| *GDI*       | Higher means item is more diagnostic. | Measures the variance of the item success probabilities based on the reduced attribute profile                                                                                        |
|             |                                       |                                                                                                                                                                                       |
|             | Cutpoint undetermined.                |                                                                                                                                                                                       |
+-------------+---------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

```{r}
#extract discrimination metrics
ecpe_discrim3 <- GDINA::extract(ecpe_est3, what = "discrim")

#print the results, arranged lowest (worst) to highest (best)
dplyr::arrange(ecpe_discrim3, `P(1)-P(0)`)
```

### Step 5: Classification Reliability

The Standards [@americaneducationalresearchassociation2014] suggest that reliability estimates of any score used should be reported. In DCM, classification reliability is measured through classification accuracy and classification consistency [@shi2021; @handbook2019].

-   classification accuracy: extent to which the estimated attribute classifications and the true classifications are the same
-   classification consistency: extent to which the estimated attribute classifications from two parallel test forms are consistent.

There are many measures of reliability in DCM [@handbook2019]. The GDINA package calculates classification accuracy at the test, pattern, and attribute levels according to Iaconangelo and Wang et al.[^2] [as cited by @shi2021].

Can specify the type of scores to estimate consistency for. `what = MAP` is for maximum a posteriori and `what = MLE` is for maximum likelihood estimation.

```{r}
#get test-level, pattern-level, and attribute-level accuracy
ecpe_acc3 <- GDINA::CA(ecpe_est3, what = "MAP")

#print all
ecpe_acc3 

#save test-level classification accuracy (Iaconangelo, 2017) into R object
ecpe_acc_test3 <- ecpe_acc3$tau

#save pattern-level classification accuracy (Iaconangelo, 2017)
ecpe_acc_pat3 <- ecpe_acc3$tau_l

#save attribute level classification accuracy (Wang et al., 2015)
ecpe_acc_att3 <- ecpe_acc3$tau_k

#conditional classification matrix (Iaconangelo, 2017)
ecpe_acc_ccm3 <- ecpe_acc3$CCM


```

[^2]: Note the CDM R package uses different metrics based on Johnson and Sinharay's work.

### Step 6: Interpret parameter estimates

If desired, we can estimate bootstrapped standard errors with the `GDINA::bootSE()` function. This may take a long time.

```{r eval=FALSE}
ecpe_est_boot3 <- GDINA::bootSE(ecpe_est3,bootsample = 5000,randomseed=123)
```

#### Item parameter estimates:

Item parameter estimates can be extracted from GDINA estimates using `stats::coef()`.

```{r}
#item probabilities of each reduced latent class with standard errors.
stats::coef(ecpe_est3, withSE = TRUE) # item probabilities of success & standard errors
```

Instead of estimated probabilities for each class, we can instead ask for the delta parameters with standard errors. I believe the metric of these will depend on the item-specific model used. The general idea is:

-   `d0` is the item intercept, corresponding to the probability/log/log-odds when no attributes are mastered (i.e., the baseline probability).
-   `d1` is the main effect for mastering attribute 1, `d2` for attribute 2, etc. In other words, they are the increase in probability/log/log-odds associated with mastery of the given attribute.
-   `d12` is the estimated two-way interaction effect between attributes 1 and 2. It demonstrates the increase in probability/log/log-odds due to the mastery of both attributes.
-   These scale up accordingly (e.g., `d23` would be two-way interaction effect between attributes 2 and 3, `d123` would be the three-way interaction between attributes 1, 2 and 3, etc.)

```{r}
#extract items with log or logit link to aid delta interpretation. item_vec is vector of item names. cbind() combines two vectors by columns. as_tibble() turns into tibble. filter() selects models without an identity link.
cbind(item_vec, GDINA::extract(ecpe_est3, what = "linkfunc")) %>% 
  as_tibble() %>% 
  dplyr::filter(V2 != "identity") %>% 
  print()

#Delta parameters with standard errors
print(stats::coef(ecpe_est3, what = "delta", withSE = TRUE))

#alternatively:
  #GDINA::extract(ecpe_est3, what = "delta.parm")
  #GDINA::extract(ecpe_est3, what = "delta.se")


```

`stats::coef(ecpe_est3, withSE = TRUE)` provides item success probabilities for *reduced* latent classes (i.e., only relevant latent classes). We can extract item success probabilities for *all* latent classes instead with `"LCprob"`:

```{r}
ecpe_est3_lcprob <- stats::coef(ecpe_est3, "LCprob")
colnames(ecpe_est3_lcprob) <- paste0("lcprob_", colnames(ecpe_est3_lcprob))
head(ecpe_est3_lcprob)
```

We can extract guessing and slip parameters with standard errors.

-   `guessing`: the probability of a student who have not mastered any required attributes.
-   `slip`: probability of failure of students who have mastered all required attributes.

```{r}
#Guessing and slip parameters with standard errors
head(stats::coef(ecpe_est3, what = "gs", withSE = TRUE))

```

We already discussed plotting the probability of correct response for a reduced latent class:

```{r}
#Item is the item(s) you want to plot
plot(ecpe_est3, item = 10, withSE = TRUE)
```

We can instead plot item response bar functions for a given item using all latent classes:

```{r}
#select an item to show
item_to_plot = 1

#create a base R barplot, subsetting the data to just include the relevant item
barplot(ecpe_est3$LC.prob[item_to_plot,],
        ylab = "Probability of Correct Response",
        xlab = "Latent Class Membership")
```

#### Person parameters: individual level

Can print the estimated attribute proficiency using a variety of methods:

-   `what = EAP` is for expected a posteriori
-   `what = MAP` is for maximum a posteriori
-   `what = MLE` is for maximum likelihood estimation.

For MAP and MLE, an additional column denotes whether there are multiple modes in the posterior or maximum likelihood function.

```{r}
# EAP estimates of attribute profiles are default. using head() to only view first 5
ecpe_est3_eap <- GDINA::personparm(ecpe_est3)
colnames(ecpe_est3_eap) <- paste0(colnames(ecpe_est3_eap), "_eap")
head(ecpe_est3_eap)

# extract MAP estimates of attribute profiles
head(GDINA::personparm(ecpe_est3, what = "MAP")) 

# MLE estimates of attribute profiles
head(GDINA::personparm(ecpe_est3, what = "MLE")) 

```

We can also extract the estimated probabilities each student had of being proficient in a given attribute:

```{r}
#mp stands for mastery probabilities
ecpe_est3_mp <- GDINA::personparm(ecpe_est3, what = "mp")
colnames(ecpe_est3_mp) <- paste0(colnames(ecpe_est3_mp), "_mp")
head(ecpe_est3_mp)
```

Plot the marginal mastery probabilities of the three attributes for select participants:

```{r}
##plot mastery probability for individual 8 
plot(ecpe_est3, what = "mp", person = 8) 

#plot the marginal mastery probability for individuals 1, 8, 28, and 31
plot(ecpe_est3, what = "mp", person = c(1, 8, 28, 31))
```

Plot individual posterior probabilities of latent class membership ==figure out how to do this as code below isn't working==

```{r}
plot(ecpe_est3, what = "posterior.prob", person = 8)

```

#### Person parameters: Aggregate level

-   Estimated proportions of latent classes indicate the proportion of students estimated to be in each latent class `what = "posterior.prob"`

```{r}
#Estimated proportions of the latent classes: attribute posterior probabilities
ecpe_est3_postprob <- GDINA::extract(ecpe_est3, what = "posterior.prob")
ecpe_est3_postprob

#prevalance of non-proficiency and proficiency in each attribute
ecpe_est3_attprev <- GDINA::extract(ecpe_est3, what = "prevalence")
ecpe_est3_attprev
#another way of getting estimated proportions of students in each latent class
  #stats::coef(ecpe_est3,"lambda")

```

We can plot the aggregate proportions of students in each latent class:

==figure out how to get this to work==

```{r}
plot(ecpe_est3, what = "posterior.prob")
```

#### Calculate correlation between attributes

We can use the psych package to calculate a tetachoric correlation matrix between attributes by extracting 1) the attribute pattern matrix and 2) the probability of students in each latent class.

```{r}
ecpe_est3_tetcor <- psych::tetrachoric(x = GDINA::extract(ecpe_est3,"attributepattern"),
                   weight = GDINA::extract(ecpe_est3,"posterior.prob"))
ecpe_est3_tetcor
```

### Step 7: Saving and Reporting Analysis

#### Saving all results in single file

We can save our entire R global environment with `save.image()`.

```{r}
save.image(file = "output/ecpe_dcm.Rdata")
```

#### Saving individual files

I'm not a fan of the above approach, as it clutters the global environment. We'll individually save various documents instead.

First, let's start by saving various dataframes as `.csv` as "R Data Serialization" `.Rds` files. The gdina object is quite complex, so saving as a `.Rds` file is easiest. We can subsequently read in `.Rds` files with either `readRDS()` or `load()` depending on your version of R and how the data was saved.

The remaining dataframes are easy to save as `.csv` files.

```{r}
#save the 'final' estimated gdina object
saveRDS(ecpe_est3, file = "output/ecpe_est3.Rds")

#save the cleaned data used in analysis
write_csv(ecpe_df, file = "clean_data/ecpe_data.csv")

#save the original and revised q matrices
write_csv(ecpe_q, file = "clean_data/ecpe_qmatrix_original.csv")
write_csv(ecpe_q_empirical, file = "clean_data/ecpe_qmatrix_revised.csv")

#save the item-specific DCMs used in model calibration
write_csv(ecpe_model_compare$selected.model, file = "output/ecpe_item_dcm.csv")

#save the test-level relative fit indices
write_csv(ecpe_tfit_relative$IC, file = "output/ecpe_tfit_relative.csv")

#save various item statistics
write_csv(cbind(ecpe_q_empirical, ecpe_model_compare$selected.model, ecpe_discrim3, ecpe_est3_lcprob), file = "output/ecpe_itemstats.csv")
```

Save some model-building and model fit analyses in a text file:

```{r}
#create a new text file named ecpe_modelbuild_modelfit.txt in the "output" folder
sink(file = "output/ecpe_modelbuild_modelfit.txt")

#print the analysis title and the system time analyzed
paste0("ecpe analysis", Sys.time())

#print some details of q-matrix evaluation.
print("##### Empirical Q-Matrix Evaluation #######")
ecpe_qval$method
print("suggested q-matrix (also saved in clean_data)")
print(as.list(cbind(item_vec, ecpe_qval$sug.Q)))

#print suggested model
print("######### Model Calibration #######")
paste0("calibration method = ", ecpe_model_compare$method)

print("######### Model Fit ###########")
print("test-level fit")
ecpe_tfit3
print("aggregated item fit (as source of test fit)")
ecpe_ifit3
sink()

#shouldn't be necessary, but closes all connections again. Use if you have any errors or difficulties.
#closeAllConnections()

```

Extract all item and structural estimates in one go. This syntax is imperfect as standard errors seem to not be saved:

```{r}
#create vector of parameters to loop over
to_extract <-  c("catprob", "delta", "gs", "itemprob", "LCprob")

#use sink() to save printed output to .txt file
sink(file = "output/ecpe_est3_estimates.txt")

#print the analysis title and the system time analyzed
paste0("ecpe analysis", Sys.time())

#document the model used
print("model = ecpe_est3")

#print general model information
print(ecpe_est3)
summary(ecpe_est3)

#print classification accuracy
print("Classification Reliability")
print(ecpe_acc3)

print("#######ITEM-LEVEL ESTIMATES###########")
#loop through, printing all estimates.
for (est in to_extract) {
  print(est)
  print(coef(ecpe_est3, what = est, withSE = TRUE))
}

print("item discrimination")
print(ecpe_discrim3)

print("############ NON-ITEM ESTIMATES ###############")

print("Attribute Correlations")
ecpe_est3_tetcor$rho
ecpe_est3_tetcor$tau

print("Attribute Prevalence")
ecpe_est3_attprev$all

print("Latent Class Proportions")
ecpe_est3_postprob
#close the connection and save the results
sink()

#shouldn't be necessary, but closes all connections again. Use if you have any errors or difficulties.
#closeAllConnections()

```

#### Save Person Parameters

We can use the data with an id variable to add the model-estimated person parameters. This is equivalent to saving factor scores or theta scores in factor analysis or IRT.

```{r}
write_csv(cbind(ecpe_id, ecpe_est3_eap, ecpe_est3_mp), file = "clean_data/ecpe_data_dcm.csv")
```

#### Publication-Ready Plots

All the plots and results thus far have been useful for us, but not publication-ready. We can use the data we extracted to generate our own plots using `ggplot2` or R's base graph capabilities.

R data visualization is a workshop all on its own, so we'll avoid that for today.

## Wrap-Up

### Summary

In this workshop, we discussed the following:

-   Conceptual foundations of DCM
    -   Conceptual ideas of DCM models
-   Running DCM in R
    -   Reading in data and q-matrix
    -   Empirical q-matrix validation
    -   Measurement model calibration
    -   Model fit evaluation
    -   Item diagnosticity
    -   Classification Reliability
    -   Parameter Estimate Interpretation
    -   Saving and Reporting DCM
-   Wrap-up and Next Steps
    -   Advanced Applications of DCM
    -   Summary
    -   Resources

### Advanced Applications of DCM

-   **"Small"** **samples:** There is increasing potential for DCM with small sample sizes by using models such as the Nonparametric Classification Method [@chiu2013] and the restricted DINA model [@nájera2023].
-   Longitudinal DCMs: Using DCM longitudinally in a manner similar to latent transition analysis but with confirmatory DCM measurement models.
-   Hierarchical DCMs: Imposing structure on the attributes such that one attribute (e.g, multiplication) can be only mastered after mastery of another (e.g., addition)
-   Polytomous DCMs: Latent attributes with more than two states (2 states: non-proficient vs proficient; 3 states: beginner vs intermediate vs advanced)
-   DIF in DCM: Investigating differential item functioning within the DCM context. See `GDINA::dif()` function.

### Further Reading

Wenchao Ma has a helpful [DCM reading list](https://dfcf59c426e6c585878c-4903f3efef28792726cbdc916f3a80da.ssl.cf2.rackcdn.com/ncme_dee9872123eaab4097da156c7d7b6148.pdf)

### Homework Activities

[@ma2019] has a great list of homework activities to solidify your understanding.

### Congratulations! 

Thank you for joining me for this workshop.
